{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "169g5NuucK2wNJXFvLqi-tlCc3sWoHEug",
      "authorship_tag": "ABX9TyPNNmlkjcLUouIF5WDOaCDn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wuyi010/model/blob/main/resnet_layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dube4JbkeE5W",
        "outputId": "8c4441c4-7b9e-4bf9-9138-150299a34c74"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Pytorch官方ResNet模型\\nfrom torchvision.models import resnet34\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "\"\"\"\n",
        "# 搭建resnet-layer模型\n",
        "#\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import torchvision.models.resnet\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"搭建BasicBlock模块\"\"\"\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channel, out_channel, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        # 使用BN层是不需要使用bias的，bias最后会抵消掉\n",
        "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1, stride=stride, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channel)    # BN层, BN层放在conv层和relu层中间使用\n",
        "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "    # 前向传播\n",
        "    def forward(self, X):\n",
        "        identity = X\n",
        "        Y = self.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "\n",
        "        if self.downsample is not None:    # 保证原始输入X的size与主分支卷积后的输出size叠加时维度相同\n",
        "            identity = self.downsample(X)\n",
        "\n",
        "        return self.relu(Y + identity)\n",
        "        \"\"\"神经网络的前向传播函数:\n",
        "            它接受一个输入张量X，然后通过一些卷积层和批量归一化层来计算输出张量Y。\n",
        "            如果存在下采样层，它将对输入张量进行下采样以使其与输出张量的尺寸相同。\n",
        "            最后，输出张量Y和输入张量X的恒等映射相加并通过ReLU激活函数进行激活。\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "class BottleNeck(nn.Module):\n",
        "    \"\"\"搭建BottleNeck模块\"\"\"\n",
        "    # BottleNeck模块最终输出out_channel是Residual模块输入in_channel的size的4倍(Residual模块输入为64)，shortcut分支in_channel\n",
        "    # 为Residual的输入64，因此需要在shortcut分支上将Residual模块的in_channel扩张4倍，使之与原始输入图片X的size一致\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_channel, out_channel, stride=1, downsample=None):\n",
        "        super(BottleNeck, self).__init__()\n",
        "        # 默认原始输入为224，经过7x7层和3x3层之后BottleNeck的输入降至64\n",
        "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channel)    # BN层, BN层放在conv层和relu层中间使用\n",
        "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
        "        self.conv3 = nn.Conv2d(out_channel, out_channel * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channel * self.expansion)  # Residual中第三层out_channel扩张到in_channel的4倍\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    # 前向传播\n",
        "    def forward(self, X):\n",
        "        identity = X\n",
        "\n",
        "        Y = self.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.relu(self.bn2(self.conv2(Y)))\n",
        "        Y = self.bn3(self.conv3(Y))\n",
        "\n",
        "        if self.downsample is not None:    # 保证原始输入X的size与主分支卷积后的输出size叠加时维度相同\n",
        "            identity = self.downsample(X)\n",
        "\n",
        "        return self.relu(Y + identity)\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"搭建ResNet-layer通用框架\"\"\"\n",
        "    # num_classes是训练集的分类个数，include_top是在ResNet的基础上搭建更加复杂的网络时用到，此处用不到\n",
        "    def __init__(self, residual, num_residuals, num_classes=1000, include_top=True):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.out_channel = 64    # 输出通道数(即卷积核个数)，会生成与设定的输出通道数相同的卷积核个数\n",
        "        self.include_top = include_top\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, self.out_channel, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)    # 3表示输入特征图像的RGB通道数为3，即图片数据的输入通道为3\n",
        "        self.bn1 = nn.BatchNorm2d(self.out_channel)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = self.residual_block(residual, 64, num_residuals[0])\n",
        "        self.conv3 = self.residual_block(residual, 128, num_residuals[1], stride=2)\n",
        "        self.conv4 = self.residual_block(residual, 256, num_residuals[2], stride=2)\n",
        "        self.conv5 = self.residual_block(residual, 512, num_residuals[3], stride=2)\n",
        "        if self.include_top:\n",
        "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))    # output_size = (1, 1)\n",
        "            self.fc = nn.Linear(512 * residual.expansion, num_classes)\n",
        "\n",
        "        # 对conv层进行初始化操作\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') #fan_out保留了向后传递中权重的大小。\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def residual_block(self, residual, channel, num_residuals, stride=1):\n",
        "        downsample = None\n",
        "\n",
        "        # 用在每个conv_x组块的第一层的shortcut分支上，此时上个conv_x输出out_channel与本conv_x所要求的输入in_channel通道数不同，\n",
        "        # 所以用downsample调整进行升维，使输出out_channel调整到本conv_x后续处理所要求的维度。\n",
        "        # 同时stride=2进行下采样减小尺寸size，(注：conv2时没有进行下采样，conv3-5进行下采样，size=56、28、14、7)。\n",
        "        if stride != 1 or self.out_channel != channel * residual.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.out_channel, channel * residual.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(channel * residual.expansion))\n",
        "\n",
        "        block = []    # block列表保存某个conv_x组块里for循环生成的所有层\n",
        "        # 添加每一个conv_x组块里的第一层，第一层决定此组块是否需要下采样(后续层不需要)\n",
        "        block.append(residual(self.out_channel, channel, downsample=downsample, stride=stride))\n",
        "        self.out_channel = channel * residual.expansion    # 输出通道out_channel扩张\n",
        "\n",
        "        for _ in range(1, num_residuals):\n",
        "            block.append(residual(self.out_channel, channel))\n",
        "\n",
        "        # 非关键字参数的特征是一个星号*加上参数名，比如*number，定义后，number可以接收任意数量的参数，并将它们储存在一个tuple中\n",
        "        return nn.Sequential(*block)\n",
        "\n",
        "    # 前向传播\n",
        "    def forward(self, X):\n",
        "        Y = self.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.maxpool(Y)\n",
        "        Y = self.conv5(self.conv4(self.conv3(self.conv2(Y))))\n",
        "\n",
        "        if self.include_top:\n",
        "            Y = self.avgpool(Y)\n",
        "            Y = torch.flatten(Y, 1)\n",
        "            Y = self.fc(Y)\n",
        "\n",
        "        return Y\n",
        "\n",
        "\n",
        "# 构建ResNet-34模型\n",
        "def resnet34(num_classes=1000, include_top=True):\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n",
        "\n",
        "\n",
        "# 构建ResNet-50模型\n",
        "def resnet50(num_classes=1000, include_top=True):\n",
        "    return ResNet(BottleNeck, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n",
        "\n",
        "\n",
        "# 模型网络结构可视化\n",
        "net = resnet34()\n",
        "#print(net)\n",
        "\n",
        "\"\"\"\n",
        "# 1. 使用torchsummary中的summary查看模型的输入输出形状、顺序结构，网络参数量，网络模型大小等信息\n",
        "from torchsummary import summary\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = net.to(device)\n",
        "summary(model, (3, 224, 224))    # 3是RGB通道数，即表示输入224 * 224的3通道的数据\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# 2. 使用torchviz中的make_dot生成模型的网络结构，pdf图包括计算路径、网络各层的权重、偏移量\n",
        "from torchviz import make_dot\n",
        "\n",
        "X = torch.rand(size=(1, 3, 224, 224))    # 3是RGB通道数，即表示输入224 * 224的3通道的数据\n",
        "Y = net(X)\n",
        "vise = make_dot(Y, params=dict(net.named_parameters()))\n",
        "vise.view()\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# Pytorch官方ResNet模型\n",
        "from torchvision.models import resnet34\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3N99S99nvF40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#迁移学习\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from tqdm import tqdm\n",
        "\n",
        "# from model import resnet34 \n",
        "\n",
        "\n",
        "def main():\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"using {} device.\".format(device))\n",
        "\n",
        "    data_transform = {\n",
        "        \"train\": transforms.Compose([transforms.RandomResizedCrop(224),\n",
        "                                     transforms.RandomHorizontalFlip(),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
        "        \"val\": transforms.Compose([transforms.Resize(256),\n",
        "                                   transforms.CenterCrop(224),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}\n",
        "\n",
        "# /content/drive/MyDrive/ColabNotebooks/data/flower_photos.tar\n",
        "    # data_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))  # get data root path\n",
        "    # image_path = os.path.join(data_root, \"data_set\", \"flower_data\")  # flower data set path\n",
        "    image_path =\"/content/drive/MyDrive/ColabNotebooks/data/flower_photos.tar\"\n",
        "    assert os.path.exists(image_path), \"{} path does not exist.\".format(image_path)\n",
        "    train_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"train\"),\n",
        "                      transform=data_transform[\"train\"])\n",
        "    train_num = len(train_dataset)\n",
        "\n",
        "    # {'daisy':0, 'dandelion':1, 'roses':2, 'sunflower':3, 'tulips':4}\n",
        "    flower_list = train_dataset.class_to_idx\n",
        "    cla_dict = dict((val, key) for key, val in flower_list.items())\n",
        "    # write dict into json file\n",
        "    json_str = json.dumps(cla_dict, indent=4)\n",
        "    with open('class_indices.json', 'w') as json_file:\n",
        "        json_file.write(json_str)\n",
        "\n",
        "    batch_size = 16\n",
        "    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\n",
        "    print('Using {} dataloader workers every process'.format(nw))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                               batch_size=batch_size, shuffle=True,\n",
        "                                               num_workers=nw)\n",
        "\n",
        "    validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"val\"),\n",
        "                                            transform=data_transform[\"val\"])\n",
        "    val_num = len(validate_dataset)\n",
        "    validate_loader = torch.utils.data.DataLoader(validate_dataset,\n",
        "                                                  batch_size=batch_size, shuffle=False,\n",
        "                                                  num_workers=nw)\n",
        "\n",
        "    print(\"using {} images for training, {} images for validation.\".format(train_num,\n",
        "                                                                           val_num))\n",
        "    \n",
        "    net = resnet34()\n",
        "    # load pretrain weights\n",
        "    # download url: https://download.pytorch.org/models/resnet34-333f7ec4.pth\n",
        "    model_weight_path = \"./resnet34-pre.pth\"\n",
        "    assert os.path.exists(model_weight_path), \"file {} does not exist.\".format(model_weight_path)\n",
        "    net.load_state_dict(torch.load(model_weight_path, map_location='cpu'))\n",
        "    # for param in net.parameters():\n",
        "    #     param.requires_grad = False\n",
        "\n",
        "    # change fc layer structure\n",
        "    in_channel = net.fc.in_features\n",
        "    net.fc = nn.Linear(in_channel, 5)\n",
        "    net.to(device)\n",
        "\n",
        "    # define loss function\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    # construct an optimizer\n",
        "    params = [p for p in net.parameters() if p.requires_grad]\n",
        "    optimizer = optim.Adam(params, lr=0.0001)\n",
        "\n",
        "    epochs = 3\n",
        "    best_acc = 0.0\n",
        "    save_path = './resNet34.pth'\n",
        "    train_steps = len(train_loader)\n",
        "    for epoch in range(epochs):\n",
        "        # train\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        train_bar = tqdm(train_loader, file=sys.stdout)\n",
        "        for step, data in enumerate(train_bar):\n",
        "            images, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            logits = net(images.to(device))\n",
        "            loss = loss_function(logits, labels.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1,\n",
        "                                                                     epochs,\n",
        "                                                                     loss)\n",
        "\n",
        "        # validate\n",
        "        net.eval()\n",
        "        acc = 0.0  # accumulate accurate number / epoch\n",
        "        with torch.no_grad():\n",
        "            val_bar = tqdm(validate_loader, file=sys.stdout)\n",
        "            for val_data in val_bar:\n",
        "                val_images, val_labels = val_data\n",
        "                outputs = net(val_images.to(device))\n",
        "                # loss = loss_function(outputs, test_labels)\n",
        "                predict_y = torch.max(outputs, dim=1)[1]\n",
        "                acc += torch.eq(predict_y, val_labels.to(device)).sum().item()\n",
        "\n",
        "                val_bar.desc = \"valid epoch[{}/{}]\".format(epoch + 1,\n",
        "                                                           epochs)\n",
        "\n",
        "        val_accurate = acc / val_num\n",
        "        print('[epoch %d] train_loss: %.3f  val_accuracy: %.3f' %\n",
        "              (epoch + 1, running_loss / train_steps, val_accurate))\n",
        "\n",
        "        if val_accurate > best_acc:\n",
        "            best_acc = val_accurate\n",
        "            torch.save(net.state_dict(), save_path)\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        },
        "id": "Kt6CPODt37MD",
        "outputId": "11b401ce-15cf-4e90-826a-025431c5aaa9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cpu device.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotADirectoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-98743f12553a>\u001b[0m in \u001b[0;36m<cell line: 135>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-98743f12553a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/ColabNotebooks/data/flower_photos.tar\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"{} path does not exist.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     train_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"train\"),\n\u001b[0m\u001b[1;32m     35\u001b[0m                       transform=data_transform[\"train\"])\n\u001b[1;32m     36\u001b[0m     \u001b[0mtrain_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     ):\n\u001b[0;32m--> 309\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    142\u001b[0m     ) -> None:\n\u001b[1;32m    143\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/content/drive/MyDrive/ColabNotebooks/data/flower_photos.tar/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "训练resnet34 + CIFAR10数据集\n",
        "# 搭建resnet-layer模型\n",
        "#\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import torchvision.models.resnet\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"搭建BasicBlock模块\"\"\"\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channel, out_channel, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        # 使用BN层是不需要使用bias的，bias最后会抵消掉\n",
        "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1, stride=stride, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channel)    # BN层, BN层放在conv层和relu层中间使用\n",
        "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "    # 前向传播\n",
        "    def forward(self, X):\n",
        "        identity = X\n",
        "        Y = self.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "\n",
        "        if self.downsample is not None:    # 保证原始输入X的size与主分支卷积后的输出size叠加时维度相同\n",
        "            identity = self.downsample(X)\n",
        "\n",
        "        return self.relu(Y + identity)\n",
        "        \"\"\"神经网络的前向传播函数:\n",
        "            它接受一个输入张量X，然后通过一些卷积层和批量归一化层来计算输出张量Y。\n",
        "            如果存在下采样层，它将对输入张量进行下采样以使其与输出张量的尺寸相同。\n",
        "            最后，输出张量Y和输入张量X的恒等映射相加并通过ReLU激活函数进行激活。\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "class BottleNeck(nn.Module):\n",
        "    \"\"\"搭建BottleNeck模块\"\"\"\n",
        "    # BottleNeck模块最终输出out_channel是Residual模块输入in_channel的size的4倍(Residual模块输入为64)，shortcut分支in_channel\n",
        "    # 为Residual的输入64，因此需要在shortcut分支上将Residual模块的in_channel扩张4倍，使之与原始输入图片X的size一致\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_channel, out_channel, stride=1, downsample=None):\n",
        "        super(BottleNeck, self).__init__()\n",
        "        # 默认原始输入为224，经过7x7层和3x3层之后BottleNeck的输入降至64\n",
        "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channel)    # BN层, BN层放在conv层和relu层中间使用\n",
        "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
        "        self.conv3 = nn.Conv2d(out_channel, out_channel * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channel * self.expansion)  # Residual中第三层out_channel扩张到in_channel的4倍\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    # 前向传播\n",
        "    def forward(self, X):\n",
        "        identity = X\n",
        "\n",
        "        Y = self.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.relu(self.bn2(self.conv2(Y)))\n",
        "        Y = self.bn3(self.conv3(Y))\n",
        "\n",
        "        if self.downsample is not None:    # 保证原始输入X的size与主分支卷积后的输出size叠加时维度相同\n",
        "            identity = self.downsample(X)\n",
        "\n",
        "        return self.relu(Y + identity)\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"搭建ResNet-layer通用框架\"\"\"\n",
        "    # num_classes是训练集的分类个数，include_top是在ResNet的基础上搭建更加复杂的网络时用到，此处用不到\n",
        "    def __init__(self, residual, num_residuals, num_classes=1000, include_top=True):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.out_channel = 64    # 输出通道数(即卷积核个数)，会生成与设定的输出通道数相同的卷积核个数\n",
        "        self.include_top = include_top\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, self.out_channel, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)    # 3表示输入特征图像的RGB通道数为3，即图片数据的输入通道为3\n",
        "        self.bn1 = nn.BatchNorm2d(self.out_channel)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = self.residual_block(residual, 64, num_residuals[0])\n",
        "        self.conv3 = self.residual_block(residual, 128, num_residuals[1], stride=2)\n",
        "        self.conv4 = self.residual_block(residual, 256, num_residuals[2], stride=2)\n",
        "        self.conv5 = self.residual_block(residual, 512, num_residuals[3], stride=2)\n",
        "        if self.include_top:\n",
        "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))    # output_size = (1, 1)\n",
        "            self.fc = nn.Linear(512 * residual.expansion, num_classes)\n",
        "\n",
        "        # 对conv层进行初始化操作\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') #fan_out保留了向后传递中权重的大小。\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def residual_block(self, residual, channel, num_residuals, stride=1):\n",
        "        downsample = None\n",
        "\n",
        "        # 用在每个conv_x组块的第一层的shortcut分支上，此时上个conv_x输出out_channel与本conv_x所要求的输入in_channel通道数不同，\n",
        "        # 所以用downsample调整进行升维，使输出out_channel调整到本conv_x后续处理所要求的维度。\n",
        "        # 同时stride=2进行下采样减小尺寸size，(注：conv2时没有进行下采样，conv3-5进行下采样，size=56、28、14、7)。\n",
        "        if stride != 1 or self.out_channel != channel * residual.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.out_channel, channel * residual.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(channel * residual.expansion))\n",
        "\n",
        "        block = []    # block列表保存某个conv_x组块里for循环生成的所有层\n",
        "        # 添加每一个conv_x组块里的第一层，第一层决定此组块是否需要下采样(后续层不需要)\n",
        "        block.append(residual(self.out_channel, channel, downsample=downsample, stride=stride))\n",
        "        self.out_channel = channel * residual.expansion    # 输出通道out_channel扩张\n",
        "\n",
        "        for _ in range(1, num_residuals):\n",
        "            block.append(residual(self.out_channel, channel))\n",
        "\n",
        "        # 非关键字参数的特征是一个星号*加上参数名，比如*number，定义后，number可以接收任意数量的参数，并将它们储存在一个tuple中\n",
        "        return nn.Sequential(*block)\n",
        "\n",
        "    # 前向传播\n",
        "    def forward(self, X):\n",
        "        Y = self.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.maxpool(Y)\n",
        "        Y = self.conv5(self.conv4(self.conv3(self.conv2(Y))))\n",
        "\n",
        "        if self.include_top:\n",
        "            Y = self.avgpool(Y)\n",
        "            Y = torch.flatten(Y, 1)\n",
        "            Y = self.fc(Y)\n",
        "\n",
        "        return Y\n",
        "\n",
        "\n",
        "# 构建ResNet-34模型\n",
        "def resnet34(num_classes=1000, include_top=True):\n",
        "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn,optim,tensor\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import  make_grid\n",
        "from torchvision import datasets,transforms\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "\n",
        "#全局变量\n",
        "batch_size=256   #每次喂入的数据量\n",
        "\n",
        "# num_print=int(50000//batch_size//4)\n",
        "num_print=100\n",
        "\n",
        "epoch_num=70 #总迭代次数\n",
        "\n",
        "lr=0.01\n",
        "step_size=10  #每n次epoch更新一次学习率\n",
        "\n",
        "#数据获取(数据增强,归一化)\n",
        "def transforms_RandomHorizontalFlip():\n",
        "\n",
        "    #transforms.Compose(),将一系列的transforms有序组合,实现按照这些方法依次对图像操作\n",
        "\n",
        "    #ToTensor()使图片数据转换为tensor张量,这个过程包含了归一化,图像数据从0~255压缩到0~1,这个函数必须在Normalize之前使用\n",
        "    #实现原理,即针对不同类型进行处理,原理即各值除以255,\n",
        "    #最后通过torch.from_numpy将PIL Image或者 numpy.ndarray()针对具体类型转成torch.tensor()数据类型\n",
        "\n",
        "    #Normalize()是归一化过程,ToTensor()的作用是将图像数据转换为(0,1)之间的张量,Normalize()则使用公式(x-mean)/std\n",
        "    #将每个元素分布到(-1,1). 归一化后数据转为标准格式,\n",
        "    transform_train=transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                      transforms.ToTensor(),\n",
        "                      transforms.Normalize((0.485,0.456,0.406),(0.229,0.224,0.225))])\n",
        "\n",
        "\n",
        "    transform=transforms.Compose([transforms.ToTensor(),\n",
        "                   transforms.Normalize((0.485,0.456,0.406),(0.226,0.224,0.225))])\n",
        "\n",
        "\n",
        "    #root:cifar-10 的根目录,data_path\n",
        "    #train:True=训练集, False=测试集\n",
        "    #transform:(可调用,可选)-接收PIL图像并返回转换版本的函数\n",
        "    #download:true=从互联网上下载数据,并将其放在root目录下,如果数据集已经下载,就什么都不干\n",
        "    train_dataset=datasets.CIFAR10(root='../../data_hub/cifar10/data_1',train=True,transform=transform_train,download=True)\n",
        "    test_dataset=datasets.CIFAR10(root='../../data_hub/cifar10/data_1',train=False,transform=transform,download=True)\n",
        "\n",
        "    return train_dataset,test_dataset\n",
        "\n",
        "#数据增强:随机翻转\n",
        "train_dataset,test_dataset=transforms_RandomHorizontalFlip()\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "#Dataloader(....)\n",
        "dataset:就是pytorch已有的数据读取接口,或者自定义的数据接口的输出,该输出要么是torch.utils.data.Dataset类的对象,\n",
        "要么是继承自torch.utils.data.Dataset类的自定义类的对象\n",
        "\n",
        "batch_size:如果有50000张训练集,则相当于把训练集平均分成(50000/batch_size)份,每份batch_size张图片\n",
        "train_loader中的每个元素相当于一个分组,一个组中batch_size图片,\n",
        "\n",
        "shuffle:设置为True时会在每个epoch重新打乱数据(默认:False),一般在训练数据中会采用\n",
        "num_workers:这个参数必须>=0,0的话表示数据导入在主进程中进行,其他大于0的数表示通过多个进程来导入数据,可以加快数据导入速度\n",
        "drop_last:设定为True如果数据集大小不能被批量大小整除的时候,将丢到最后一个不完整的batch(默认为False)\n",
        "'''\n",
        "\n",
        "train_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
        "test_loader=DataLoader(test_dataset,batch_size=batch_size,shuffle=False)\n",
        "\n",
        "#模型,优化器\n",
        "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#from VggNet import *\n",
        "\n",
        "model=resnet34().to(device)\n",
        "\n",
        "#在多分类情况下一般使用交叉熵\n",
        "# torch.nn.CrossEntropyLoss相当于softmax + log + nllloss。\n",
        "# 预测的概率大于1不符合预期，可以使用softmax归一，取log后是交叉熵，取负号是为了符合loss越小，预测概率越大。\n",
        "# 在实际训练中，如果做的是分类任务，且使用CrossEntropyLoss作为损失函数的话，\n",
        "# 神经网络的部分就没必要加入nn.Softmax或者nn.LogSoftmax等之类的，因为在CrossEntropyLoss已经内置了该功能。\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "'''\n",
        "params(iterable)-待优化参数的iterable或者定义了参数组的dict\n",
        "lr(float):学习率\n",
        "\n",
        "momentum(float)-动量因子\n",
        "\n",
        "weight_decay(float):权重衰减,使用的目的是防止过拟合.在损失函数中,weight decay是放在正则项前面的一个系数,正则项一般指示模型的复杂度\n",
        "所以weight decay的作用是调节模型复杂度对损失函数的影响,若weight decay很大,则复杂的模型损失函数的值也就大.\n",
        "\n",
        "dampening:动量的有抑制因子\n",
        "\n",
        "optimizer.param_group:是长度为2的list,其中的元素是两个字典\n",
        "optimzer.param_group:长度为6的字典,包括['amsgrad','params','lr','weight_decay',eps']\n",
        "optimzer.param_group:表示优化器状态的一个字典\n",
        "\n",
        "'''\n",
        "optimizer=optim.SGD(model.parameters(),lr=lr,momentum=0.8,weight_decay=0.001) #神经网络优化器\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "scheduler 调整学习率而设置，我这里设置的gamma衰减率为0.5，step_size为10，也就是每10个epoch将学习率衰减至原来的0.5倍。\n",
        "optimizer(Optimizer):要更改学习率的优化器\n",
        "milestones(list):递增的list,存放要更新的lr的epoch\n",
        "gamma:(float):更新lr的乘法因子\n",
        "last_epoch:：最后一个epoch的index，如果是训练了很多个epoch后中断了，继续训练，这个值就等于加载的模型的epoch。\n",
        "默认为-1表示从头开始训练，即从epoch=1\n",
        "'''\n",
        "schedule=optim.lr_scheduler.StepLR(optimizer,step_size=step_size,gamma=0.5,last_epoch=-1)\n",
        "\n",
        "\n",
        "\n",
        "#训练\n",
        "loss_list=[]  #为了后续画出损失图\n",
        "start=time.time()\n",
        "\n",
        "#train\n",
        "for epoch in range(epoch_num):\n",
        "    ww = 0\n",
        "    running_loss=0.0\n",
        "    #0是对i的给值(循环次数从0开始计数还是从1开始计数的问题):\n",
        "    #???\n",
        "    for i,(inputs,labels) in enumerate(train_loader,0):\n",
        "\n",
        "        #将数据从train_loader中读出来,一次读取的样本是32个\n",
        "        inputs,labels=inputs.to(device),labels.to(device)\n",
        "\n",
        "        #用于梯度清零,在每次应用新的梯度时,要把原来的梯度清零,否则梯度会累加\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "\n",
        "        loss=criterion(outputs,labels).to(device)\n",
        "\n",
        "        #反向传播,pytorch会自动计算反向传播的值\n",
        "        loss.backward()\n",
        "        #对反向传播以后对目标函数进行优化\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        running_loss+=loss.item()\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "        if(i+1)%num_print==0:\n",
        "            print('[%d epoch,%d]  loss:%.6f' %(epoch+1,i+1,running_loss/num_print))\n",
        "            running_loss=0.0\n",
        "\n",
        "    lr_1=optimizer.param_groups[0]['lr'] #返回优化器的第一个参数组的学习率\n",
        "    print(\"learn_rate:%.15f\"%lr_1)\n",
        "    schedule.step() #10 per epoch\n",
        "\n",
        "end=time.time()\n",
        "print(\"time:{}\".format(end-start))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#测试\n",
        "\n",
        "#由于训练集不需要梯度更新,于是进入测试模式\n",
        "model.eval()\n",
        "correct=0.0\n",
        "total=0\n",
        "with torch.no_grad(): #训练集不需要反向传播\n",
        "    print(\"=======================test=======================\")\n",
        "    for inputs,labels in test_loader:\n",
        "        inputs,labels=inputs.to(device),labels.to(device)\n",
        "        outputs=model(inputs)\n",
        "\n",
        "        pred=outputs.argmax(dim=1)  #返回每一行中最大值元素索引\n",
        "        total+=inputs.size(0)    #输入张量的第一维度的大小\n",
        "        correct+=torch.eq(pred,labels).sum().item() #sum()返回张量中所有元素的总和，item()返回标量张量的值\n",
        "\n",
        "print(\"Accuracy of the network on the 10000 test images:%.2f %%\" %(100*correct/total) )\n",
        "print(\"===============================================\")\n",
        "\n",
        "# # PATH = './content/drive/MyDrive/Colab Notebooks/model'  \n",
        "PATH = '/content/drive/MyDrive/ColabNotebooks/modelSave/ResNet34'\n",
        "# torch.save(model.state_dict(), PATH)\n",
        "torch.save({\n",
        "       'epoch': epoch,\n",
        "       'model_state_dict': model.state_dict(),\n",
        "       'optimizer_state_dict': optimizer.state_dict(),\n",
        "       'loss': loss,\n",
        "      \n",
        "      }, PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sc2sKMTatCs7",
        "outputId": "914de2d1-a408-43d9-e4a1-0299eda1b075"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../data_hub/cifar10/data_1/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:13<00:00, 12962754.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data_hub/cifar10/data_1/cifar-10-python.tar.gz to ../../data_hub/cifar10/data_1\n",
            "Files already downloaded and verified\n",
            "[1 epoch,100]  loss:1.986396\n",
            "learn_rate:0.010000000000000\n",
            "[2 epoch,100]  loss:1.377919\n",
            "learn_rate:0.010000000000000\n",
            "[3 epoch,100]  loss:1.180463\n",
            "learn_rate:0.010000000000000\n",
            "[4 epoch,100]  loss:1.042457\n",
            "learn_rate:0.010000000000000\n",
            "[5 epoch,100]  loss:0.929018\n",
            "learn_rate:0.010000000000000\n",
            "[6 epoch,100]  loss:0.841195\n",
            "learn_rate:0.010000000000000\n",
            "[7 epoch,100]  loss:0.757213\n",
            "learn_rate:0.010000000000000\n",
            "[8 epoch,100]  loss:0.696968\n",
            "learn_rate:0.010000000000000\n",
            "[9 epoch,100]  loss:0.633942\n",
            "learn_rate:0.010000000000000\n",
            "[10 epoch,100]  loss:0.574235\n",
            "learn_rate:0.010000000000000\n",
            "[11 epoch,100]  loss:0.449523\n",
            "learn_rate:0.005000000000000\n",
            "[12 epoch,100]  loss:0.367346\n",
            "learn_rate:0.005000000000000\n",
            "[13 epoch,100]  loss:0.316708\n",
            "learn_rate:0.005000000000000\n",
            "[14 epoch,100]  loss:0.287442\n",
            "learn_rate:0.005000000000000\n",
            "[15 epoch,100]  loss:0.254513\n",
            "learn_rate:0.005000000000000\n",
            "[16 epoch,100]  loss:0.240061\n",
            "learn_rate:0.005000000000000\n",
            "[17 epoch,100]  loss:0.216981\n",
            "learn_rate:0.005000000000000\n",
            "[18 epoch,100]  loss:0.199735\n",
            "learn_rate:0.005000000000000\n",
            "[19 epoch,100]  loss:0.184522\n",
            "learn_rate:0.005000000000000\n",
            "[20 epoch,100]  loss:0.178005\n",
            "learn_rate:0.005000000000000\n",
            "[21 epoch,100]  loss:0.110794\n",
            "learn_rate:0.002500000000000\n",
            "[22 epoch,100]  loss:0.058208\n",
            "learn_rate:0.002500000000000\n",
            "[23 epoch,100]  loss:0.042699\n",
            "learn_rate:0.002500000000000\n",
            "[24 epoch,100]  loss:0.032792\n",
            "learn_rate:0.002500000000000\n",
            "[25 epoch,100]  loss:0.026553\n",
            "learn_rate:0.002500000000000\n",
            "[26 epoch,100]  loss:0.017514\n",
            "learn_rate:0.002500000000000\n",
            "[27 epoch,100]  loss:0.016174\n",
            "learn_rate:0.002500000000000\n",
            "[28 epoch,100]  loss:0.010840\n",
            "learn_rate:0.002500000000000\n",
            "[29 epoch,100]  loss:0.008930\n",
            "learn_rate:0.002500000000000\n",
            "[30 epoch,100]  loss:0.009288\n",
            "learn_rate:0.002500000000000\n",
            "[31 epoch,100]  loss:0.005999\n",
            "learn_rate:0.001250000000000\n",
            "[32 epoch,100]  loss:0.005519\n",
            "learn_rate:0.001250000000000\n",
            "[33 epoch,100]  loss:0.003596\n",
            "learn_rate:0.001250000000000\n",
            "[34 epoch,100]  loss:0.003170\n",
            "learn_rate:0.001250000000000\n",
            "[35 epoch,100]  loss:0.002741\n",
            "learn_rate:0.001250000000000\n",
            "[36 epoch,100]  loss:0.002789\n",
            "learn_rate:0.001250000000000\n",
            "[37 epoch,100]  loss:0.002646\n",
            "learn_rate:0.001250000000000\n",
            "[38 epoch,100]  loss:0.002010\n",
            "learn_rate:0.001250000000000\n",
            "[39 epoch,100]  loss:0.002613\n",
            "learn_rate:0.001250000000000\n",
            "[40 epoch,100]  loss:0.002247\n",
            "learn_rate:0.001250000000000\n",
            "[41 epoch,100]  loss:0.001985\n",
            "learn_rate:0.000625000000000\n",
            "[42 epoch,100]  loss:0.001683\n",
            "learn_rate:0.000625000000000\n",
            "[43 epoch,100]  loss:0.001752\n",
            "learn_rate:0.000625000000000\n",
            "[44 epoch,100]  loss:0.001685\n",
            "learn_rate:0.000625000000000\n",
            "[45 epoch,100]  loss:0.001758\n",
            "learn_rate:0.000625000000000\n",
            "[46 epoch,100]  loss:0.001487\n",
            "learn_rate:0.000625000000000\n",
            "[47 epoch,100]  loss:0.001563\n",
            "learn_rate:0.000625000000000\n",
            "[48 epoch,100]  loss:0.001610\n",
            "learn_rate:0.000625000000000\n",
            "[49 epoch,100]  loss:0.001409\n",
            "learn_rate:0.000625000000000\n",
            "[50 epoch,100]  loss:0.001439\n",
            "learn_rate:0.000625000000000\n",
            "[51 epoch,100]  loss:0.001312\n",
            "learn_rate:0.000312500000000\n",
            "[52 epoch,100]  loss:0.001402\n",
            "learn_rate:0.000312500000000\n",
            "[53 epoch,100]  loss:0.001447\n",
            "learn_rate:0.000312500000000\n",
            "[54 epoch,100]  loss:0.001233\n",
            "learn_rate:0.000312500000000\n",
            "[55 epoch,100]  loss:0.001257\n",
            "learn_rate:0.000312500000000\n",
            "[56 epoch,100]  loss:0.001311\n",
            "learn_rate:0.000312500000000\n",
            "[57 epoch,100]  loss:0.001257\n",
            "learn_rate:0.000312500000000\n",
            "[58 epoch,100]  loss:0.001266\n",
            "learn_rate:0.000312500000000\n",
            "[59 epoch,100]  loss:0.001183\n",
            "learn_rate:0.000312500000000\n",
            "[60 epoch,100]  loss:0.001104\n",
            "learn_rate:0.000312500000000\n",
            "[61 epoch,100]  loss:0.001175\n",
            "learn_rate:0.000156250000000\n",
            "[62 epoch,100]  loss:0.001187\n",
            "learn_rate:0.000156250000000\n",
            "[63 epoch,100]  loss:0.001228\n",
            "learn_rate:0.000156250000000\n",
            "[64 epoch,100]  loss:0.001208\n",
            "learn_rate:0.000156250000000\n",
            "[65 epoch,100]  loss:0.001236\n",
            "learn_rate:0.000156250000000\n",
            "[66 epoch,100]  loss:0.001119\n",
            "learn_rate:0.000156250000000\n",
            "[67 epoch,100]  loss:0.001225\n",
            "learn_rate:0.000156250000000\n",
            "[68 epoch,100]  loss:0.001138\n",
            "learn_rate:0.000156250000000\n",
            "[69 epoch,100]  loss:0.001132\n",
            "learn_rate:0.000156250000000\n",
            "[70 epoch,100]  loss:0.001097\n",
            "learn_rate:0.000156250000000\n",
            "time:1908.5356271266937\n",
            "=======================test=======================\n",
            "Accuracy of the network on the 10000 test images:72.03 %\n",
            "===============================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# 判断是否有GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "num_epochs = 100 #50轮\n",
        "batch_size = 100 #50步长\n",
        "learning_rate = 0.01 #学习率0.01\n",
        "\n",
        "# 图像预处理\n",
        "transform = transforms.Compose([\n",
        "                transforms.Pad(4),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomCrop(32),\n",
        "                transforms.ToTensor()])\n",
        "\n",
        "# CIFAR-10 数据集下载\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='../data/',\n",
        "                        train=True, \n",
        "                        transform=transform,\n",
        "                        download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='../data/',\n",
        "                       train=False, \n",
        "                       transform=transforms.ToTensor())\n",
        "\n",
        "# 数据载入\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                       batch_size=batch_size,\n",
        "                       shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                      batch_size=batch_size,\n",
        "                      shuffle=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"搭建BasicBlock模块\"\"\"\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channel, out_channel, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        # 使用BN层是不需要使用bias的，bias最后会抵消掉\n",
        "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1, stride=stride, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channel)    # BN层, BN层放在conv层和relu层中间使用\n",
        "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "    # 前向传播\n",
        "    def forward(self, X):\n",
        "        identity = X\n",
        "        Y = self.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "\n",
        "        if self.downsample is not None:    # 保证原始输入X的size与主分支卷积后的输出size叠加时维度相同\n",
        "            identity = self.downsample(X)\n",
        "\n",
        "        return self.relu(Y + identity)\n",
        "        \"\"\"神经网络的前向传播函数:\n",
        "            它接受一个输入张量X，然后通过一些卷积层和批量归一化层来计算输出张量Y。\n",
        "            如果存在下采样层，它将对输入张量进行下采样以使其与输出张量的尺寸相同。\n",
        "            最后，输出张量Y和输入张量X的恒等映射相加并通过ReLU激活函数进行激活。\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "class BottleNeck(nn.Module):\n",
        "    \"\"\"搭建BottleNeck模块\"\"\"\n",
        "    # BottleNeck模块最终输出out_channel是Residual模块输入in_channel的size的4倍(Residual模块输入为64)，shortcut分支in_channel\n",
        "    # 为Residual的输入64，因此需要在shortcut分支上将Residual模块的in_channel扩张4倍，使之与原始输入图片X的size一致\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_channel, out_channel, stride=1, downsample=None):\n",
        "        super(BottleNeck, self).__init__()\n",
        "        # 默认原始输入为224，经过7x7层和3x3层之后BottleNeck的输入降至64\n",
        "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channel)    # BN层, BN层放在conv层和relu层中间使用\n",
        "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
        "        self.conv3 = nn.Conv2d(out_channel, out_channel * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channel * self.expansion)  # Residual中第三层out_channel扩张到in_channel的4倍\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    # 前向传播\n",
        "    def forward(self, X):\n",
        "        identity = X\n",
        "\n",
        "        Y = self.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.relu(self.bn2(self.conv2(Y)))\n",
        "        Y = self.bn3(self.conv3(Y))\n",
        "\n",
        "        if self.downsample is not None:    # 保证原始输入X的size与主分支卷积后的输出size叠加时维度相同\n",
        "            identity = self.downsample(X)\n",
        "\n",
        "        return self.relu(Y + identity)\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"搭建ResNet-layer通用框架\"\"\"\n",
        "    # num_classes是训练集的分类个数，include_top是在ResNet的基础上搭建更加复杂的网络时用到，此处用不到\n",
        "    def __init__(self, residual, num_residuals, num_classes=1000, include_top=True):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.out_channel = 64    # 输出通道数(即卷积核个数)，会生成与设定的输出通道数相同的卷积核个数\n",
        "        self.include_top = include_top\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, self.out_channel, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)    # 3表示输入特征图像的RGB通道数为3，即图片数据的输入通道为3\n",
        "        self.bn1 = nn.BatchNorm2d(self.out_channel)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = self.residual_block(residual, 64, num_residuals[0])\n",
        "        self.conv3 = self.residual_block(residual, 128, num_residuals[1], stride=2)\n",
        "        self.conv4 = self.residual_block(residual, 256, num_residuals[2], stride=2)\n",
        "        self.conv5 = self.residual_block(residual, 512, num_residuals[3], stride=2)\n",
        "        if self.include_top:\n",
        "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))    # output_size = (1, 1)\n",
        "            self.fc = nn.Linear(512 * residual.expansion, num_classes)\n",
        "\n",
        "        # 对conv层进行初始化操作\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') #fan_out保留了向后传递中权重的大小。\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def residual_block(self, residual, channel, num_residuals, stride=1):\n",
        "        downsample = None\n",
        "\n",
        "        # 用在每个conv_x组块的第一层的shortcut分支上，此时上个conv_x输出out_channel与本conv_x所要求的输入in_channel通道数不同，\n",
        "        # 所以用downsample调整进行升维，使输出out_channel调整到本conv_x后续处理所要求的维度。\n",
        "        # 同时stride=2进行下采样减小尺寸size，(注：conv2时没有进行下采样，conv3-5进行下采样，size=56、28、14、7)。\n",
        "        if stride != 1 or self.out_channel != channel * residual.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.out_channel, channel * residual.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(channel * residual.expansion))\n",
        "\n",
        "        block = []    # block列表保存某个conv_x组块里for循环生成的所有层\n",
        "        # 添加每一个conv_x组块里的第一层，第一层决定此组块是否需要下采样(后续层不需要)\n",
        "        block.append(residual(self.out_channel, channel, downsample=downsample, stride=stride))\n",
        "        self.out_channel = channel * residual.expansion    # 输出通道out_channel扩张\n",
        "\n",
        "        for _ in range(1, num_residuals):\n",
        "            block.append(residual(self.out_channel, channel))\n",
        "\n",
        "        # 非关键字参数的特征是一个星号*加上参数名，比如*number，定义后，number可以接收任意数量的参数，并将它们储存在一个tuple中\n",
        "        return nn.Sequential(*block)\n",
        "\n",
        "    # 前向传播\n",
        "    def forward(self, X):\n",
        "        Y = self.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.maxpool(Y)\n",
        "        Y = self.conv5(self.conv4(self.conv3(self.conv2(Y))))\n",
        "\n",
        "        if self.include_top:\n",
        "            Y = self.avgpool(Y)\n",
        "            Y = torch.flatten(Y, 1)\n",
        "            Y = self.fc(Y)\n",
        "\n",
        "        return Y\n",
        "\n",
        "\n",
        "# # 构建ResNet-34模型\n",
        "# def resnet34(num_classes=1000, include_top=True):\n",
        "#     return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n",
        "# # \n",
        "\n",
        "# 构建ResNet-50模型\n",
        "def resnet50(num_classes=1000, include_top=True):\n",
        "    return ResNet(BottleNeck, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n",
        "#模型,优化器\n",
        "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "model = resnet50().to(device)\n",
        "\n",
        "\n",
        "# 损失函数\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 更新学习率\n",
        "def update_lr(optimizer, lr):    \n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "# 训练数据集\n",
        "total_step = len(train_loader)\n",
        "curr_lr = learning_rate\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "    # 延迟学习率\n",
        "    if (epoch+1) % 20 == 0:\n",
        "        curr_lr /= 3\n",
        "        update_lr(optimizer, curr_lr)\n",
        "\n",
        "# 测试网络模型\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        \n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
        "\n",
        "# S将模型保存\n",
        "torch.save(model.state_dict(), 'resnet.ckpt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQWjige_D37x",
        "outputId": "2695e9bd-ae67-42cc-ec05-523f5833c9f8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Epoch [1/100], Step [100/500] Loss: 3.4539\n",
            "Epoch [1/100], Step [200/500] Loss: 2.2621\n",
            "Epoch [1/100], Step [300/500] Loss: 2.1389\n",
            "Epoch [1/100], Step [400/500] Loss: 1.9445\n",
            "Epoch [1/100], Step [500/500] Loss: 1.9467\n",
            "Epoch [2/100], Step [100/500] Loss: 1.8993\n",
            "Epoch [2/100], Step [200/500] Loss: 1.6854\n",
            "Epoch [2/100], Step [300/500] Loss: 1.6963\n",
            "Epoch [2/100], Step [400/500] Loss: 1.9156\n",
            "Epoch [2/100], Step [500/500] Loss: 1.7383\n",
            "Epoch [3/100], Step [100/500] Loss: 1.5354\n",
            "Epoch [3/100], Step [200/500] Loss: 1.8040\n",
            "Epoch [3/100], Step [300/500] Loss: 1.5674\n",
            "Epoch [3/100], Step [400/500] Loss: 1.5697\n",
            "Epoch [3/100], Step [500/500] Loss: 1.5785\n",
            "Epoch [4/100], Step [100/500] Loss: 1.4129\n",
            "Epoch [4/100], Step [200/500] Loss: 1.2668\n",
            "Epoch [4/100], Step [300/500] Loss: 1.3072\n",
            "Epoch [4/100], Step [400/500] Loss: 1.2381\n",
            "Epoch [4/100], Step [500/500] Loss: 1.4077\n",
            "Epoch [5/100], Step [100/500] Loss: 1.2668\n",
            "Epoch [5/100], Step [200/500] Loss: 1.2989\n",
            "Epoch [5/100], Step [300/500] Loss: 1.4786\n",
            "Epoch [5/100], Step [400/500] Loss: 1.3924\n",
            "Epoch [5/100], Step [500/500] Loss: 1.2431\n",
            "Epoch [6/100], Step [100/500] Loss: 1.3178\n",
            "Epoch [6/100], Step [200/500] Loss: 0.9894\n",
            "Epoch [6/100], Step [300/500] Loss: 1.2222\n",
            "Epoch [6/100], Step [400/500] Loss: 0.9779\n",
            "Epoch [6/100], Step [500/500] Loss: 1.2887\n",
            "Epoch [7/100], Step [100/500] Loss: 1.1890\n",
            "Epoch [7/100], Step [200/500] Loss: 1.1981\n",
            "Epoch [7/100], Step [300/500] Loss: 1.3093\n",
            "Epoch [7/100], Step [400/500] Loss: 1.2115\n",
            "Epoch [7/100], Step [500/500] Loss: 0.9758\n",
            "Epoch [8/100], Step [100/500] Loss: 0.8626\n",
            "Epoch [8/100], Step [200/500] Loss: 1.2218\n",
            "Epoch [8/100], Step [300/500] Loss: 0.9133\n",
            "Epoch [8/100], Step [400/500] Loss: 0.9434\n",
            "Epoch [8/100], Step [500/500] Loss: 0.8026\n",
            "Epoch [9/100], Step [100/500] Loss: 1.0327\n",
            "Epoch [9/100], Step [200/500] Loss: 0.9796\n",
            "Epoch [9/100], Step [300/500] Loss: 1.0030\n",
            "Epoch [9/100], Step [400/500] Loss: 0.9334\n",
            "Epoch [9/100], Step [500/500] Loss: 0.7397\n",
            "Epoch [10/100], Step [100/500] Loss: 0.9597\n",
            "Epoch [10/100], Step [200/500] Loss: 0.8623\n",
            "Epoch [10/100], Step [300/500] Loss: 0.7930\n",
            "Epoch [10/100], Step [400/500] Loss: 0.8517\n",
            "Epoch [10/100], Step [500/500] Loss: 0.8155\n",
            "Epoch [11/100], Step [100/500] Loss: 0.8579\n",
            "Epoch [11/100], Step [200/500] Loss: 0.7485\n",
            "Epoch [11/100], Step [300/500] Loss: 1.0145\n",
            "Epoch [11/100], Step [400/500] Loss: 0.7876\n",
            "Epoch [11/100], Step [500/500] Loss: 1.0533\n",
            "Epoch [12/100], Step [100/500] Loss: 0.8318\n",
            "Epoch [12/100], Step [200/500] Loss: 0.8240\n",
            "Epoch [12/100], Step [300/500] Loss: 0.6796\n",
            "Epoch [12/100], Step [400/500] Loss: 0.9275\n",
            "Epoch [12/100], Step [500/500] Loss: 0.7809\n",
            "Epoch [13/100], Step [100/500] Loss: 0.7002\n",
            "Epoch [13/100], Step [200/500] Loss: 0.6155\n",
            "Epoch [13/100], Step [300/500] Loss: 0.7648\n",
            "Epoch [13/100], Step [400/500] Loss: 0.7816\n",
            "Epoch [13/100], Step [500/500] Loss: 0.8048\n",
            "Epoch [14/100], Step [100/500] Loss: 0.6334\n",
            "Epoch [14/100], Step [200/500] Loss: 0.6950\n",
            "Epoch [14/100], Step [300/500] Loss: 0.8670\n",
            "Epoch [14/100], Step [400/500] Loss: 0.8716\n",
            "Epoch [14/100], Step [500/500] Loss: 0.6615\n",
            "Epoch [15/100], Step [100/500] Loss: 0.6569\n",
            "Epoch [15/100], Step [200/500] Loss: 0.6801\n",
            "Epoch [15/100], Step [300/500] Loss: 0.6607\n",
            "Epoch [15/100], Step [400/500] Loss: 0.6651\n",
            "Epoch [15/100], Step [500/500] Loss: 0.7271\n",
            "Epoch [16/100], Step [100/500] Loss: 0.8344\n",
            "Epoch [16/100], Step [200/500] Loss: 0.5425\n",
            "Epoch [16/100], Step [300/500] Loss: 0.7287\n",
            "Epoch [16/100], Step [400/500] Loss: 0.7688\n",
            "Epoch [16/100], Step [500/500] Loss: 0.8915\n",
            "Epoch [17/100], Step [100/500] Loss: 0.6032\n",
            "Epoch [17/100], Step [200/500] Loss: 0.7277\n",
            "Epoch [17/100], Step [300/500] Loss: 0.6371\n",
            "Epoch [17/100], Step [400/500] Loss: 0.6458\n",
            "Epoch [17/100], Step [500/500] Loss: 0.7832\n",
            "Epoch [18/100], Step [100/500] Loss: 0.5951\n",
            "Epoch [18/100], Step [200/500] Loss: 0.7062\n",
            "Epoch [18/100], Step [300/500] Loss: 0.6603\n",
            "Epoch [18/100], Step [400/500] Loss: 0.7520\n",
            "Epoch [18/100], Step [500/500] Loss: 0.6160\n",
            "Epoch [19/100], Step [100/500] Loss: 0.4692\n",
            "Epoch [19/100], Step [200/500] Loss: 0.5588\n",
            "Epoch [19/100], Step [300/500] Loss: 0.5520\n",
            "Epoch [19/100], Step [400/500] Loss: 0.6081\n",
            "Epoch [19/100], Step [500/500] Loss: 0.6480\n",
            "Epoch [20/100], Step [100/500] Loss: 0.5189\n",
            "Epoch [20/100], Step [200/500] Loss: 0.4863\n",
            "Epoch [20/100], Step [300/500] Loss: 0.5898\n",
            "Epoch [20/100], Step [400/500] Loss: 0.5782\n",
            "Epoch [20/100], Step [500/500] Loss: 0.5901\n",
            "Epoch [21/100], Step [100/500] Loss: 0.4988\n",
            "Epoch [21/100], Step [200/500] Loss: 0.4527\n",
            "Epoch [21/100], Step [300/500] Loss: 0.5812\n",
            "Epoch [21/100], Step [400/500] Loss: 0.3896\n",
            "Epoch [21/100], Step [500/500] Loss: 0.4910\n",
            "Epoch [22/100], Step [100/500] Loss: 0.4924\n",
            "Epoch [22/100], Step [200/500] Loss: 0.4412\n",
            "Epoch [22/100], Step [300/500] Loss: 0.5249\n",
            "Epoch [22/100], Step [400/500] Loss: 0.5038\n",
            "Epoch [22/100], Step [500/500] Loss: 0.3495\n",
            "Epoch [23/100], Step [100/500] Loss: 0.5108\n",
            "Epoch [23/100], Step [200/500] Loss: 0.4158\n",
            "Epoch [23/100], Step [300/500] Loss: 0.4891\n",
            "Epoch [23/100], Step [400/500] Loss: 0.6744\n",
            "Epoch [23/100], Step [500/500] Loss: 0.4396\n",
            "Epoch [24/100], Step [100/500] Loss: 0.4881\n",
            "Epoch [24/100], Step [200/500] Loss: 0.4668\n",
            "Epoch [24/100], Step [300/500] Loss: 0.4505\n",
            "Epoch [24/100], Step [400/500] Loss: 0.4979\n",
            "Epoch [24/100], Step [500/500] Loss: 0.3183\n",
            "Epoch [25/100], Step [100/500] Loss: 0.2881\n",
            "Epoch [25/100], Step [200/500] Loss: 0.4885\n",
            "Epoch [25/100], Step [300/500] Loss: 0.4768\n",
            "Epoch [25/100], Step [400/500] Loss: 0.3228\n",
            "Epoch [25/100], Step [500/500] Loss: 0.3627\n",
            "Epoch [26/100], Step [100/500] Loss: 0.3453\n",
            "Epoch [26/100], Step [200/500] Loss: 0.3511\n",
            "Epoch [26/100], Step [300/500] Loss: 0.2819\n",
            "Epoch [26/100], Step [400/500] Loss: 0.3490\n",
            "Epoch [26/100], Step [500/500] Loss: 0.2317\n",
            "Epoch [27/100], Step [100/500] Loss: 0.5077\n",
            "Epoch [27/100], Step [200/500] Loss: 0.3500\n",
            "Epoch [27/100], Step [300/500] Loss: 0.3424\n",
            "Epoch [27/100], Step [400/500] Loss: 0.2485\n",
            "Epoch [27/100], Step [500/500] Loss: 0.3438\n",
            "Epoch [28/100], Step [100/500] Loss: 0.3677\n",
            "Epoch [28/100], Step [200/500] Loss: 0.5225\n",
            "Epoch [28/100], Step [300/500] Loss: 0.3481\n",
            "Epoch [28/100], Step [400/500] Loss: 0.4037\n",
            "Epoch [28/100], Step [500/500] Loss: 0.3270\n",
            "Epoch [29/100], Step [100/500] Loss: 0.2967\n",
            "Epoch [29/100], Step [200/500] Loss: 0.3484\n",
            "Epoch [29/100], Step [300/500] Loss: 0.3282\n",
            "Epoch [29/100], Step [400/500] Loss: 0.3839\n",
            "Epoch [29/100], Step [500/500] Loss: 0.2768\n",
            "Epoch [30/100], Step [100/500] Loss: 0.3504\n",
            "Epoch [30/100], Step [200/500] Loss: 0.2952\n",
            "Epoch [30/100], Step [300/500] Loss: 0.4796\n",
            "Epoch [30/100], Step [400/500] Loss: 0.2876\n",
            "Epoch [30/100], Step [500/500] Loss: 0.1987\n",
            "Epoch [31/100], Step [100/500] Loss: 0.4509\n",
            "Epoch [31/100], Step [200/500] Loss: 0.2392\n",
            "Epoch [31/100], Step [300/500] Loss: 0.3292\n",
            "Epoch [31/100], Step [400/500] Loss: 0.2602\n",
            "Epoch [31/100], Step [500/500] Loss: 0.4963\n",
            "Epoch [32/100], Step [100/500] Loss: 0.2827\n",
            "Epoch [32/100], Step [200/500] Loss: 0.1985\n",
            "Epoch [32/100], Step [300/500] Loss: 0.4416\n",
            "Epoch [32/100], Step [400/500] Loss: 0.3012\n",
            "Epoch [32/100], Step [500/500] Loss: 0.3387\n",
            "Epoch [33/100], Step [100/500] Loss: 0.2040\n",
            "Epoch [33/100], Step [200/500] Loss: 0.4050\n",
            "Epoch [33/100], Step [300/500] Loss: 0.2010\n",
            "Epoch [33/100], Step [400/500] Loss: 0.4876\n",
            "Epoch [33/100], Step [500/500] Loss: 0.2457\n",
            "Epoch [34/100], Step [100/500] Loss: 0.2729\n",
            "Epoch [34/100], Step [200/500] Loss: 0.2987\n",
            "Epoch [34/100], Step [300/500] Loss: 0.2874\n",
            "Epoch [34/100], Step [400/500] Loss: 0.1924\n",
            "Epoch [34/100], Step [500/500] Loss: 0.3674\n",
            "Epoch [35/100], Step [100/500] Loss: 0.3355\n",
            "Epoch [35/100], Step [200/500] Loss: 0.2429\n",
            "Epoch [35/100], Step [300/500] Loss: 0.2122\n",
            "Epoch [35/100], Step [400/500] Loss: 0.3697\n",
            "Epoch [35/100], Step [500/500] Loss: 0.3998\n",
            "Epoch [36/100], Step [100/500] Loss: 0.3161\n",
            "Epoch [36/100], Step [200/500] Loss: 0.3284\n",
            "Epoch [36/100], Step [300/500] Loss: 0.3442\n",
            "Epoch [36/100], Step [400/500] Loss: 0.3382\n",
            "Epoch [36/100], Step [500/500] Loss: 0.3456\n",
            "Epoch [37/100], Step [100/500] Loss: 0.3286\n",
            "Epoch [37/100], Step [200/500] Loss: 0.2446\n",
            "Epoch [37/100], Step [300/500] Loss: 0.2823\n",
            "Epoch [37/100], Step [400/500] Loss: 0.2959\n",
            "Epoch [37/100], Step [500/500] Loss: 0.2430\n",
            "Epoch [38/100], Step [100/500] Loss: 0.1831\n",
            "Epoch [38/100], Step [200/500] Loss: 0.2889\n",
            "Epoch [38/100], Step [300/500] Loss: 0.2922\n",
            "Epoch [38/100], Step [400/500] Loss: 0.2054\n",
            "Epoch [38/100], Step [500/500] Loss: 0.1347\n",
            "Epoch [39/100], Step [100/500] Loss: 0.2637\n",
            "Epoch [39/100], Step [200/500] Loss: 0.2637\n",
            "Epoch [39/100], Step [300/500] Loss: 0.2813\n",
            "Epoch [39/100], Step [400/500] Loss: 0.3248\n",
            "Epoch [39/100], Step [500/500] Loss: 0.3057\n",
            "Epoch [40/100], Step [100/500] Loss: 0.3299\n",
            "Epoch [40/100], Step [200/500] Loss: 0.1976\n",
            "Epoch [40/100], Step [300/500] Loss: 0.2750\n",
            "Epoch [40/100], Step [400/500] Loss: 0.2294\n",
            "Epoch [40/100], Step [500/500] Loss: 0.4134\n",
            "Epoch [41/100], Step [100/500] Loss: 0.1358\n",
            "Epoch [41/100], Step [200/500] Loss: 0.1992\n",
            "Epoch [41/100], Step [300/500] Loss: 0.2670\n",
            "Epoch [41/100], Step [400/500] Loss: 0.1885\n",
            "Epoch [41/100], Step [500/500] Loss: 0.2023\n",
            "Epoch [42/100], Step [100/500] Loss: 0.2311\n",
            "Epoch [42/100], Step [200/500] Loss: 0.1840\n",
            "Epoch [42/100], Step [300/500] Loss: 0.1439\n",
            "Epoch [42/100], Step [400/500] Loss: 0.0960\n",
            "Epoch [42/100], Step [500/500] Loss: 0.1500\n",
            "Epoch [43/100], Step [100/500] Loss: 0.1447\n",
            "Epoch [43/100], Step [200/500] Loss: 0.2738\n",
            "Epoch [43/100], Step [300/500] Loss: 0.2176\n",
            "Epoch [43/100], Step [400/500] Loss: 0.1427\n",
            "Epoch [43/100], Step [500/500] Loss: 0.3577\n",
            "Epoch [44/100], Step [100/500] Loss: 0.2053\n",
            "Epoch [44/100], Step [200/500] Loss: 0.2539\n",
            "Epoch [44/100], Step [300/500] Loss: 0.1235\n",
            "Epoch [44/100], Step [400/500] Loss: 0.1545\n",
            "Epoch [44/100], Step [500/500] Loss: 0.1845\n",
            "Epoch [45/100], Step [100/500] Loss: 0.1383\n",
            "Epoch [45/100], Step [200/500] Loss: 0.1825\n",
            "Epoch [45/100], Step [300/500] Loss: 0.1985\n",
            "Epoch [45/100], Step [400/500] Loss: 0.1863\n",
            "Epoch [45/100], Step [500/500] Loss: 0.2869\n",
            "Epoch [46/100], Step [100/500] Loss: 0.1188\n",
            "Epoch [46/100], Step [200/500] Loss: 0.2223\n",
            "Epoch [46/100], Step [300/500] Loss: 0.1885\n",
            "Epoch [46/100], Step [400/500] Loss: 0.2495\n",
            "Epoch [46/100], Step [500/500] Loss: 0.1616\n",
            "Epoch [47/100], Step [100/500] Loss: 0.1949\n",
            "Epoch [47/100], Step [200/500] Loss: 0.1859\n",
            "Epoch [47/100], Step [300/500] Loss: 0.1515\n",
            "Epoch [47/100], Step [400/500] Loss: 0.1591\n",
            "Epoch [47/100], Step [500/500] Loss: 0.2433\n",
            "Epoch [48/100], Step [100/500] Loss: 0.1027\n",
            "Epoch [48/100], Step [200/500] Loss: 0.2360\n",
            "Epoch [48/100], Step [300/500] Loss: 0.1523\n",
            "Epoch [48/100], Step [400/500] Loss: 0.1851\n",
            "Epoch [48/100], Step [500/500] Loss: 0.2373\n",
            "Epoch [49/100], Step [100/500] Loss: 0.2149\n",
            "Epoch [49/100], Step [200/500] Loss: 0.1576\n",
            "Epoch [49/100], Step [300/500] Loss: 0.2118\n",
            "Epoch [49/100], Step [400/500] Loss: 0.1306\n",
            "Epoch [49/100], Step [500/500] Loss: 0.2522\n",
            "Epoch [50/100], Step [100/500] Loss: 0.1517\n",
            "Epoch [50/100], Step [200/500] Loss: 0.1229\n",
            "Epoch [50/100], Step [300/500] Loss: 0.2115\n",
            "Epoch [50/100], Step [400/500] Loss: 0.1866\n",
            "Epoch [50/100], Step [500/500] Loss: 0.1937\n",
            "Epoch [51/100], Step [100/500] Loss: 0.0771\n",
            "Epoch [51/100], Step [200/500] Loss: 0.1709\n",
            "Epoch [51/100], Step [300/500] Loss: 0.0762\n",
            "Epoch [51/100], Step [400/500] Loss: 0.2029\n",
            "Epoch [51/100], Step [500/500] Loss: 0.2107\n",
            "Epoch [52/100], Step [100/500] Loss: 0.1359\n",
            "Epoch [52/100], Step [200/500] Loss: 0.1093\n",
            "Epoch [52/100], Step [300/500] Loss: 0.2465\n",
            "Epoch [52/100], Step [400/500] Loss: 0.1144\n",
            "Epoch [52/100], Step [500/500] Loss: 0.1223\n",
            "Epoch [53/100], Step [100/500] Loss: 0.2360\n",
            "Epoch [53/100], Step [200/500] Loss: 0.2001\n",
            "Epoch [53/100], Step [300/500] Loss: 0.1507\n",
            "Epoch [53/100], Step [400/500] Loss: 0.1354\n",
            "Epoch [53/100], Step [500/500] Loss: 0.0672\n",
            "Epoch [54/100], Step [100/500] Loss: 0.1606\n",
            "Epoch [54/100], Step [200/500] Loss: 0.1224\n",
            "Epoch [54/100], Step [300/500] Loss: 0.1440\n",
            "Epoch [54/100], Step [400/500] Loss: 0.1685\n",
            "Epoch [54/100], Step [500/500] Loss: 0.0730\n",
            "Epoch [55/100], Step [100/500] Loss: 0.1313\n",
            "Epoch [55/100], Step [200/500] Loss: 0.1417\n",
            "Epoch [55/100], Step [300/500] Loss: 0.1049\n",
            "Epoch [55/100], Step [400/500] Loss: 0.1555\n",
            "Epoch [55/100], Step [500/500] Loss: 0.1537\n",
            "Epoch [56/100], Step [100/500] Loss: 0.1117\n",
            "Epoch [56/100], Step [200/500] Loss: 0.1110\n",
            "Epoch [56/100], Step [300/500] Loss: 0.1006\n",
            "Epoch [56/100], Step [400/500] Loss: 0.0682\n",
            "Epoch [56/100], Step [500/500] Loss: 0.0651\n",
            "Epoch [57/100], Step [100/500] Loss: 0.1128\n",
            "Epoch [57/100], Step [200/500] Loss: 0.1063\n",
            "Epoch [57/100], Step [300/500] Loss: 0.1105\n",
            "Epoch [57/100], Step [400/500] Loss: 0.1036\n",
            "Epoch [57/100], Step [500/500] Loss: 0.1679\n",
            "Epoch [58/100], Step [100/500] Loss: 0.1007\n",
            "Epoch [58/100], Step [200/500] Loss: 0.1191\n",
            "Epoch [58/100], Step [300/500] Loss: 0.1294\n",
            "Epoch [58/100], Step [400/500] Loss: 0.2413\n",
            "Epoch [58/100], Step [500/500] Loss: 0.0889\n",
            "Epoch [59/100], Step [100/500] Loss: 0.0999\n",
            "Epoch [59/100], Step [200/500] Loss: 0.0499\n",
            "Epoch [59/100], Step [300/500] Loss: 0.1013\n",
            "Epoch [59/100], Step [400/500] Loss: 0.1115\n",
            "Epoch [59/100], Step [500/500] Loss: 0.1661\n",
            "Epoch [60/100], Step [100/500] Loss: 0.1560\n",
            "Epoch [60/100], Step [200/500] Loss: 0.1925\n",
            "Epoch [60/100], Step [300/500] Loss: 0.1581\n",
            "Epoch [60/100], Step [400/500] Loss: 0.1439\n",
            "Epoch [60/100], Step [500/500] Loss: 0.1154\n",
            "Epoch [61/100], Step [100/500] Loss: 0.2569\n",
            "Epoch [61/100], Step [200/500] Loss: 0.1078\n",
            "Epoch [61/100], Step [300/500] Loss: 0.0877\n",
            "Epoch [61/100], Step [400/500] Loss: 0.1099\n",
            "Epoch [61/100], Step [500/500] Loss: 0.0899\n",
            "Epoch [62/100], Step [100/500] Loss: 0.0964\n",
            "Epoch [62/100], Step [200/500] Loss: 0.1246\n",
            "Epoch [62/100], Step [300/500] Loss: 0.0749\n",
            "Epoch [62/100], Step [400/500] Loss: 0.0714\n",
            "Epoch [62/100], Step [500/500] Loss: 0.0901\n",
            "Epoch [63/100], Step [100/500] Loss: 0.0609\n",
            "Epoch [63/100], Step [200/500] Loss: 0.1960\n",
            "Epoch [63/100], Step [300/500] Loss: 0.1205\n",
            "Epoch [63/100], Step [400/500] Loss: 0.0620\n",
            "Epoch [63/100], Step [500/500] Loss: 0.1255\n",
            "Epoch [64/100], Step [100/500] Loss: 0.1876\n",
            "Epoch [64/100], Step [200/500] Loss: 0.1295\n",
            "Epoch [64/100], Step [300/500] Loss: 0.0590\n",
            "Epoch [64/100], Step [400/500] Loss: 0.0793\n",
            "Epoch [64/100], Step [500/500] Loss: 0.1002\n",
            "Epoch [65/100], Step [100/500] Loss: 0.2150\n",
            "Epoch [65/100], Step [200/500] Loss: 0.0326\n",
            "Epoch [65/100], Step [300/500] Loss: 0.1861\n",
            "Epoch [65/100], Step [400/500] Loss: 0.1031\n",
            "Epoch [65/100], Step [500/500] Loss: 0.1454\n",
            "Epoch [66/100], Step [100/500] Loss: 0.0689\n",
            "Epoch [66/100], Step [200/500] Loss: 0.0719\n",
            "Epoch [66/100], Step [300/500] Loss: 0.0597\n",
            "Epoch [66/100], Step [400/500] Loss: 0.0602\n",
            "Epoch [66/100], Step [500/500] Loss: 0.1433\n",
            "Epoch [67/100], Step [100/500] Loss: 0.0953\n",
            "Epoch [67/100], Step [200/500] Loss: 0.1073\n",
            "Epoch [67/100], Step [300/500] Loss: 0.0680\n",
            "Epoch [67/100], Step [400/500] Loss: 0.0823\n",
            "Epoch [67/100], Step [500/500] Loss: 0.0892\n",
            "Epoch [68/100], Step [100/500] Loss: 0.2156\n",
            "Epoch [68/100], Step [200/500] Loss: 0.0446\n",
            "Epoch [68/100], Step [300/500] Loss: 0.1711\n",
            "Epoch [68/100], Step [400/500] Loss: 0.1158\n",
            "Epoch [68/100], Step [500/500] Loss: 0.1250\n",
            "Epoch [69/100], Step [100/500] Loss: 0.0426\n",
            "Epoch [69/100], Step [200/500] Loss: 0.0771\n",
            "Epoch [69/100], Step [300/500] Loss: 0.0550\n",
            "Epoch [69/100], Step [400/500] Loss: 0.0578\n",
            "Epoch [69/100], Step [500/500] Loss: 0.1375\n",
            "Epoch [70/100], Step [100/500] Loss: 0.1879\n",
            "Epoch [70/100], Step [200/500] Loss: 0.0512\n",
            "Epoch [70/100], Step [300/500] Loss: 0.1090\n",
            "Epoch [70/100], Step [400/500] Loss: 0.0961\n",
            "Epoch [70/100], Step [500/500] Loss: 0.0386\n",
            "Epoch [71/100], Step [100/500] Loss: 0.0693\n",
            "Epoch [71/100], Step [200/500] Loss: 0.1158\n",
            "Epoch [71/100], Step [300/500] Loss: 0.0912\n",
            "Epoch [71/100], Step [400/500] Loss: 0.0801\n",
            "Epoch [71/100], Step [500/500] Loss: 0.0794\n",
            "Epoch [72/100], Step [100/500] Loss: 0.0783\n",
            "Epoch [72/100], Step [200/500] Loss: 0.1552\n",
            "Epoch [72/100], Step [300/500] Loss: 0.0723\n",
            "Epoch [72/100], Step [400/500] Loss: 0.1312\n",
            "Epoch [72/100], Step [500/500] Loss: 0.1046\n",
            "Epoch [73/100], Step [100/500] Loss: 0.1013\n",
            "Epoch [73/100], Step [200/500] Loss: 0.1167\n",
            "Epoch [73/100], Step [300/500] Loss: 0.1507\n",
            "Epoch [73/100], Step [400/500] Loss: 0.0667\n",
            "Epoch [73/100], Step [500/500] Loss: 0.1218\n",
            "Epoch [74/100], Step [100/500] Loss: 0.0567\n",
            "Epoch [74/100], Step [200/500] Loss: 0.2422\n",
            "Epoch [74/100], Step [300/500] Loss: 0.1259\n",
            "Epoch [74/100], Step [400/500] Loss: 0.1205\n",
            "Epoch [74/100], Step [500/500] Loss: 0.0674\n",
            "Epoch [75/100], Step [100/500] Loss: 0.0994\n",
            "Epoch [75/100], Step [200/500] Loss: 0.1299\n",
            "Epoch [75/100], Step [300/500] Loss: 0.0912\n",
            "Epoch [75/100], Step [400/500] Loss: 0.1207\n",
            "Epoch [75/100], Step [500/500] Loss: 0.0578\n",
            "Epoch [76/100], Step [100/500] Loss: 0.0552\n",
            "Epoch [76/100], Step [200/500] Loss: 0.0384\n",
            "Epoch [76/100], Step [300/500] Loss: 0.0857\n",
            "Epoch [76/100], Step [400/500] Loss: 0.0657\n",
            "Epoch [76/100], Step [500/500] Loss: 0.0637\n",
            "Epoch [77/100], Step [100/500] Loss: 0.0678\n",
            "Epoch [77/100], Step [200/500] Loss: 0.0634\n",
            "Epoch [77/100], Step [300/500] Loss: 0.0950\n",
            "Epoch [77/100], Step [400/500] Loss: 0.0603\n",
            "Epoch [77/100], Step [500/500] Loss: 0.0629\n",
            "Epoch [78/100], Step [100/500] Loss: 0.0983\n",
            "Epoch [78/100], Step [200/500] Loss: 0.0542\n",
            "Epoch [78/100], Step [300/500] Loss: 0.0846\n",
            "Epoch [78/100], Step [400/500] Loss: 0.0606\n",
            "Epoch [78/100], Step [500/500] Loss: 0.1125\n",
            "Epoch [79/100], Step [100/500] Loss: 0.0699\n",
            "Epoch [79/100], Step [200/500] Loss: 0.0235\n",
            "Epoch [79/100], Step [300/500] Loss: 0.0966\n",
            "Epoch [79/100], Step [400/500] Loss: 0.0849\n",
            "Epoch [79/100], Step [500/500] Loss: 0.0624\n",
            "Epoch [80/100], Step [100/500] Loss: 0.0660\n",
            "Epoch [80/100], Step [200/500] Loss: 0.1813\n",
            "Epoch [80/100], Step [300/500] Loss: 0.0333\n",
            "Epoch [80/100], Step [400/500] Loss: 0.0469\n",
            "Epoch [80/100], Step [500/500] Loss: 0.1246\n",
            "Epoch [81/100], Step [100/500] Loss: 0.0513\n",
            "Epoch [81/100], Step [200/500] Loss: 0.0651\n",
            "Epoch [81/100], Step [300/500] Loss: 0.1127\n",
            "Epoch [81/100], Step [400/500] Loss: 0.0491\n",
            "Epoch [81/100], Step [500/500] Loss: 0.0271\n",
            "Epoch [82/100], Step [100/500] Loss: 0.0385\n",
            "Epoch [82/100], Step [200/500] Loss: 0.0877\n",
            "Epoch [82/100], Step [300/500] Loss: 0.0868\n",
            "Epoch [82/100], Step [400/500] Loss: 0.0862\n",
            "Epoch [82/100], Step [500/500] Loss: 0.0514\n",
            "Epoch [83/100], Step [100/500] Loss: 0.1079\n",
            "Epoch [83/100], Step [200/500] Loss: 0.1079\n",
            "Epoch [83/100], Step [300/500] Loss: 0.0810\n",
            "Epoch [83/100], Step [400/500] Loss: 0.2013\n",
            "Epoch [83/100], Step [500/500] Loss: 0.1534\n",
            "Epoch [84/100], Step [100/500] Loss: 0.0816\n",
            "Epoch [84/100], Step [200/500] Loss: 0.0512\n",
            "Epoch [84/100], Step [300/500] Loss: 0.0975\n",
            "Epoch [84/100], Step [400/500] Loss: 0.0453\n",
            "Epoch [84/100], Step [500/500] Loss: 0.0597\n",
            "Epoch [85/100], Step [100/500] Loss: 0.0694\n",
            "Epoch [85/100], Step [200/500] Loss: 0.0334\n",
            "Epoch [85/100], Step [300/500] Loss: 0.0552\n",
            "Epoch [85/100], Step [400/500] Loss: 0.0376\n",
            "Epoch [85/100], Step [500/500] Loss: 0.0983\n",
            "Epoch [86/100], Step [100/500] Loss: 0.1971\n",
            "Epoch [86/100], Step [200/500] Loss: 0.0499\n",
            "Epoch [86/100], Step [300/500] Loss: 0.1320\n",
            "Epoch [86/100], Step [400/500] Loss: 0.0893\n",
            "Epoch [86/100], Step [500/500] Loss: 0.0165\n",
            "Epoch [87/100], Step [100/500] Loss: 0.0550\n",
            "Epoch [87/100], Step [200/500] Loss: 0.1400\n",
            "Epoch [87/100], Step [300/500] Loss: 0.1504\n",
            "Epoch [87/100], Step [400/500] Loss: 0.1126\n",
            "Epoch [87/100], Step [500/500] Loss: 0.0849\n",
            "Epoch [88/100], Step [100/500] Loss: 0.0722\n",
            "Epoch [88/100], Step [200/500] Loss: 0.0432\n",
            "Epoch [88/100], Step [300/500] Loss: 0.0264\n",
            "Epoch [88/100], Step [400/500] Loss: 0.0291\n",
            "Epoch [88/100], Step [500/500] Loss: 0.0781\n",
            "Epoch [89/100], Step [100/500] Loss: 0.0334\n",
            "Epoch [89/100], Step [200/500] Loss: 0.0843\n",
            "Epoch [89/100], Step [300/500] Loss: 0.0124\n",
            "Epoch [89/100], Step [400/500] Loss: 0.1015\n",
            "Epoch [89/100], Step [500/500] Loss: 0.0411\n",
            "Epoch [90/100], Step [100/500] Loss: 0.0632\n",
            "Epoch [90/100], Step [200/500] Loss: 0.0322\n",
            "Epoch [90/100], Step [300/500] Loss: 0.0825\n",
            "Epoch [90/100], Step [400/500] Loss: 0.0725\n",
            "Epoch [90/100], Step [500/500] Loss: 0.0681\n",
            "Epoch [91/100], Step [100/500] Loss: 0.0541\n",
            "Epoch [91/100], Step [200/500] Loss: 0.0671\n",
            "Epoch [91/100], Step [300/500] Loss: 0.0544\n",
            "Epoch [91/100], Step [400/500] Loss: 0.1105\n",
            "Epoch [91/100], Step [500/500] Loss: 0.1484\n",
            "Epoch [92/100], Step [100/500] Loss: 0.0360\n",
            "Epoch [92/100], Step [200/500] Loss: 0.0634\n",
            "Epoch [92/100], Step [300/500] Loss: 0.0755\n",
            "Epoch [92/100], Step [400/500] Loss: 0.0541\n",
            "Epoch [92/100], Step [500/500] Loss: 0.0423\n",
            "Epoch [93/100], Step [100/500] Loss: 0.1114\n",
            "Epoch [93/100], Step [200/500] Loss: 0.1141\n",
            "Epoch [93/100], Step [300/500] Loss: 0.0463\n",
            "Epoch [93/100], Step [400/500] Loss: 0.0178\n",
            "Epoch [93/100], Step [500/500] Loss: 0.0927\n",
            "Epoch [94/100], Step [100/500] Loss: 0.0761\n",
            "Epoch [94/100], Step [200/500] Loss: 0.0592\n",
            "Epoch [94/100], Step [300/500] Loss: 0.0156\n",
            "Epoch [94/100], Step [400/500] Loss: 0.1192\n",
            "Epoch [94/100], Step [500/500] Loss: 0.0439\n",
            "Epoch [95/100], Step [100/500] Loss: 0.0184\n",
            "Epoch [95/100], Step [200/500] Loss: 0.0842\n",
            "Epoch [95/100], Step [300/500] Loss: 0.1108\n",
            "Epoch [95/100], Step [400/500] Loss: 0.0444\n",
            "Epoch [95/100], Step [500/500] Loss: 0.0460\n",
            "Epoch [96/100], Step [100/500] Loss: 0.0723\n",
            "Epoch [96/100], Step [200/500] Loss: 0.0646\n",
            "Epoch [96/100], Step [300/500] Loss: 0.0748\n",
            "Epoch [96/100], Step [400/500] Loss: 0.2077\n",
            "Epoch [96/100], Step [500/500] Loss: 0.0878\n",
            "Epoch [97/100], Step [100/500] Loss: 0.0183\n",
            "Epoch [97/100], Step [200/500] Loss: 0.1441\n",
            "Epoch [97/100], Step [300/500] Loss: 0.0589\n",
            "Epoch [97/100], Step [400/500] Loss: 0.0780\n",
            "Epoch [97/100], Step [500/500] Loss: 0.0795\n",
            "Epoch [98/100], Step [100/500] Loss: 0.0525\n",
            "Epoch [98/100], Step [200/500] Loss: 0.0571\n",
            "Epoch [98/100], Step [300/500] Loss: 0.0585\n",
            "Epoch [98/100], Step [400/500] Loss: 0.1118\n",
            "Epoch [98/100], Step [500/500] Loss: 0.0249\n",
            "Epoch [99/100], Step [100/500] Loss: 0.0374\n",
            "Epoch [99/100], Step [200/500] Loss: 0.0762\n",
            "Epoch [99/100], Step [300/500] Loss: 0.0949\n",
            "Epoch [99/100], Step [400/500] Loss: 0.1005\n",
            "Epoch [99/100], Step [500/500] Loss: 0.0400\n",
            "Epoch [100/100], Step [100/500] Loss: 0.0816\n",
            "Epoch [100/100], Step [200/500] Loss: 0.0797\n",
            "Epoch [100/100], Step [300/500] Loss: 0.0366\n",
            "Epoch [100/100], Step [400/500] Loss: 0.0397\n",
            "Epoch [100/100], Step [500/500] Loss: 0.0542\n",
            "Accuracy of the model on the test images: 86.67 %\n"
          ]
        }
      ]
    }
  ]
}